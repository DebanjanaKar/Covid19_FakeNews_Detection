{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "@author: debanjana\n",
    "\"\"\"\n",
    "\n",
    "import pickle as pkl\n",
    "with open('./resources/covid_en_tweet.pickle', 'rb') as pkl_in:\n",
    "    tweets_en = pkl.load(pkl_in)\n",
    "#loading bengali data\n",
    "with open('./resources/covid_bn_tweet.pickle', 'rb') as pkl_in:\n",
    "    tweets_bn = pkl.load(pkl_in)\n",
    "with open('./resources/covid_bn_tweet_test.pickle', 'rb') as pkl_in:\n",
    "    tweets_bn_test = pkl.load(pkl_in)\n",
    "#loading hindi data\n",
    "with open('./resources/covid_hi_tweet.pickle', 'rb') as pkl_in:\n",
    "    tweets_hi = pkl.load(pkl_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textural features\n",
    "def get_text_ft(tweets):\n",
    "    text_ft = []\n",
    "    len_ = tweets['text_info']['len']\n",
    "    upp = tweets['text_info']['num_of_upper_char']\n",
    "    qmark = tweets['text_info']['num_of_qmarks']\n",
    "    exclm = tweets['text_info']['num_of_exclmmarks']\n",
    "    urls = tweets['text'].apply(lambda x: x.count('URL'))\n",
    "    for i in range(len(tweets['text'])):\n",
    "        #print(i)\n",
    "        text_ft.append([len_[i], upp[i], qmark[i], exclm[i], urls[i]])    \n",
    "    return text_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ft_en = get_text_ft(tweets_en)\n",
    "#text_ft_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  labels\n",
      "0    for the average american the best way to tell ...       0\n",
      "1                             this is fucking bullshit       0\n",
      "2    can y ’ all please just follow the government ...       0\n",
      "3    no offense but the corona virus disappearing b...       0\n",
      "4    this is the face of someone who just spent  9 ...       1\n",
      "..                                                 ...     ...\n",
      "499  president trump ' s comments about the coronav...       1\n",
      "500  drug companies reportedly killed a provision i...       1\n",
      "501  does @ twitter have a team fighting co vid  - ...       0\n",
      "502  fox to cause pandemic  fox news is spreading  ...       1\n",
      "503  nurtw lagos is already sens it i zing their pe...       1\n",
      "\n",
      "[504 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "del tweets_en['text_info']\n",
    "del tweets_bn['text_info']\n",
    "del tweets_bn_test['text_info']\n",
    "del tweets_hi['text_info']\n",
    "df_en = pd.DataFrame(tweets_en)\n",
    "df_bn = pd.DataFrame(tweets_bn)\n",
    "df_bn_test = pd.DataFrame(tweets_bn_test)\n",
    "df_hi = pd.DataFrame(tweets_hi)\n",
    "print(df_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged = pd.concat([df_en, df_bn, df_hi, df_bn_test])\n",
    "df_merged = df_en\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, eval_df = train_test_split(df_merged, test_size=0.2)\n",
    "eval_df, test_df = train_test_split(eval_df, test_size=0.5)\n",
    "\n",
    "train_data = train_df['text'].to_list()\n",
    "eval_data = eval_df['text'].to_list()\n",
    "test_data = test_df['text'].to_list()\n",
    "\n",
    "train_tags = train_df['labels'].to_list()\n",
    "eval_tags = eval_df['labels'].to_list()\n",
    "test_tags = test_df['labels'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train, valid, test =  403 50 51\n",
      "Samples :  a state of emergency has been declared for la city and county due to confirmed cases of coronavirus . angelenos need to be prepared , but not panicked . listen to the advice of experts . make sure you ' re following @cdcgov and @la public health for the latest info and recommendations . <link> 1\n"
     ]
    }
   ],
   "source": [
    "assert len(train_data) == len(train_tags)\n",
    "assert len(eval_data) == len(eval_tags)\n",
    "assert len(test_data) == len(test_tags)\n",
    "\n",
    "print('Length of train, valid, test = ', len(train_data), len(eval_data), len(test_data))\n",
    "print('Samples : ', test_data[0], test_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('./resources/tweets_preprocessed_multi.pickle', 'wb') as pkl_out:\n",
    "    pkl.dump(train_data, pkl_out)\n",
    "    pkl.dump(eval_data, pkl_out)\n",
    "    pkl.dump(test_data, pkl_out)\n",
    "    \n",
    "    pkl.dump(train_tags, pkl_out)\n",
    "    pkl.dump(eval_tags, pkl_out)\n",
    "    pkl.dump(test_tags, pkl_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debanjana_ibm/anaconda3/envs/covidfake/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import BertEmbeddings\n",
    "embeddings = BertEmbeddings('bert-base-multilingual-uncased', layers='-1') # use only last layers\n",
    "import numpy as np\n",
    "def get_word_embeddings(sent_list):\n",
    "    \n",
    "    word_embeds = {}\n",
    "    for i, sent in enumerate(sent_list):\n",
    "        sentence = Sentence(sent)\n",
    "        embeddings.embed(sentence)\n",
    "        for word in sent.split():\n",
    "            try:\n",
    "                word_embeds[word+ '_' + str(i)] = [np.asarray(token.embedding) for token in sentence if str(token).split(' ')[-1].strip() == word.strip()][0]\n",
    "            except Exception as e:\n",
    "                print(str(e), word)\n",
    "                word_embeds[word+ '_' + str(i)] = np.zeros(768)\n",
    "    \n",
    "    return word_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n",
      "list index out of range \n"
     ]
    }
   ],
   "source": [
    "words = get_word_embeddings(train_data + eval_data + test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = list(words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/tweet_word_emb_en.pickle', 'wb') as pkl_out:\n",
    "    pkl.dump(words_list, pkl_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/keyphrases_embeddings.pickle', 'rb') as pkl_in:\n",
    "    en_embeds =  pkl.load(pkl_in)\n",
    "    bn_embeds1 = pkl.load(pkl_in)\n",
    "    bn_embeds2 = pkl.load(pkl_in)\n",
    "    hi_embeds = pkl.load(pkl_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1333, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "keyphr_embeds = en_embeds + bn_embeds1 + hi_embeds +  bn_embeds2\n",
    "keyphr_embeds = np.asarray([emb.detach().numpy() for emb in keyphr_embeds])\n",
    "keyphr_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./resources/multi_raw_outputs.pickle', 'rb') as pkl_in:\n",
    "    wt_dict = pkl.load(pkl_in)\n",
    "#print(wt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12508efab3294a3e95de84d9b2c2ad3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2f5be9127b4293a2326a9a4d9e1887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=13.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.572136  ,  2.574471  ],\n",
       "       [-2.4838808 ,  2.4175797 ],\n",
       "       [-2.357317  ,  2.3285203 ],\n",
       "       [-2.3505762 ,  2.2907462 ],\n",
       "       [-0.50719357,  0.46243185],\n",
       "       [-2.5837748 ,  2.526768  ],\n",
       "       [-2.5599816 ,  2.4843316 ],\n",
       "       [ 1.4463357 , -1.3085309 ],\n",
       "       [ 1.1202005 , -1.0068094 ],\n",
       "       [ 1.3785052 , -1.240451  ],\n",
       "       [-2.5774004 ,  2.5625243 ],\n",
       "       [-0.30324373,  0.27542344],\n",
       "       [-2.5102234 ,  2.4450874 ],\n",
       "       [-2.5673108 ,  2.5722244 ],\n",
       "       [-1.9969274 ,  1.9005226 ],\n",
       "       [-2.5359445 ,  2.5432196 ],\n",
       "       [-2.1424928 ,  2.0144618 ],\n",
       "       [ 1.3975838 , -1.3008662 ],\n",
       "       [ 1.3536893 , -1.238526  ],\n",
       "       [-2.4330568 ,  2.3929024 ],\n",
       "       [-2.1610408 ,  2.1246333 ],\n",
       "       [-2.487532  ,  2.4530842 ],\n",
       "       [-1.3435651 ,  1.1356248 ],\n",
       "       [-1.5721275 ,  1.5309868 ],\n",
       "       [ 1.7831495 , -1.5740577 ],\n",
       "       [ 0.9207386 , -0.8900965 ],\n",
       "       [-1.7828465 ,  1.5735496 ],\n",
       "       [-2.5099256 ,  2.5033908 ],\n",
       "       [-2.1470275 ,  1.9384038 ],\n",
       "       [ 0.42286664, -0.43348244],\n",
       "       [-2.1440477 ,  1.9620334 ],\n",
       "       [-2.468933  ,  2.4435158 ],\n",
       "       [-2.3868556 ,  2.314342  ],\n",
       "       [-2.3041635 ,  2.2208135 ],\n",
       "       [ 1.3266221 , -1.2160423 ],\n",
       "       [-1.9549823 ,  1.9420253 ],\n",
       "       [ 1.1097153 , -1.0287893 ],\n",
       "       [-2.5464916 ,  2.553938  ],\n",
       "       [ 0.20870332, -0.23267926],\n",
       "       [-2.3195875 ,  2.3103738 ],\n",
       "       [-2.3081052 ,  2.2902641 ],\n",
       "       [-2.036061  ,  1.9171377 ],\n",
       "       [-2.5001905 ,  2.4791112 ],\n",
       "       [-2.1534846 ,  2.1734645 ],\n",
       "       [-2.208844  ,  2.151722  ],\n",
       "       [-0.00505121, -0.15439631],\n",
       "       [-2.1178262 ,  2.0318084 ],\n",
       "       [-2.3454928 ,  2.3226733 ],\n",
       "       [-2.5199358 ,  2.5256119 ],\n",
       "       [ 1.3357462 , -1.2129092 ],\n",
       "       [ 1.2164779 , -1.1437413 ],\n",
       "       [ 1.3266774 , -1.218334  ],\n",
       "       [-2.4197443 ,  2.3690662 ],\n",
       "       [-2.4591541 ,  2.438333  ],\n",
       "       [-1.5014286 ,  1.5398268 ],\n",
       "       [-2.4386644 ,  2.3990967 ],\n",
       "       [-2.6009989 ,  2.5553598 ],\n",
       "       [ 1.7187893 , -1.3953339 ],\n",
       "       [-1.0714225 ,  0.99376476],\n",
       "       [-2.3645403 ,  2.2796073 ],\n",
       "       [-1.5845194 ,  1.4307767 ],\n",
       "       [-2.468622  ,  2.4720843 ],\n",
       "       [ 1.2595575 , -1.1778804 ],\n",
       "       [-2.4417663 ,  2.4877992 ],\n",
       "       [-2.5422459 ,  2.4937148 ],\n",
       "       [ 1.3418623 , -1.2262864 ],\n",
       "       [ 1.360947  , -1.2540985 ],\n",
       "       [-1.537478  ,  1.4769464 ],\n",
       "       [ 0.17997839, -0.05083388],\n",
       "       [-2.5472677 ,  2.5310187 ],\n",
       "       [-1.5095497 ,  1.372168  ],\n",
       "       [-2.4407778 ,  2.3532734 ],\n",
       "       [-2.1361575 ,  2.0744278 ],\n",
       "       [ 1.0526572 , -0.9635339 ],\n",
       "       [-2.5833938 ,  2.4981775 ],\n",
       "       [ 1.4936525 , -1.3713447 ],\n",
       "       [ 1.3998836 , -1.302612  ],\n",
       "       [-2.0911918 ,  2.0946772 ],\n",
       "       [ 1.378767  , -1.264157  ],\n",
       "       [-1.4461911 ,  1.411414  ],\n",
       "       [-0.6405514 ,  0.51303977],\n",
       "       [-0.83684003,  0.8338173 ],\n",
       "       [-2.4895542 ,  2.4785411 ],\n",
       "       [ 1.4246016 , -1.3220693 ],\n",
       "       [-0.6676813 ,  0.7181099 ],\n",
       "       [-2.2641768 ,  2.1977983 ],\n",
       "       [-2.527529  ,  2.506648  ],\n",
       "       [ 1.5568224 , -1.420755  ],\n",
       "       [ 1.6050855 , -1.4424721 ],\n",
       "       [ 1.8863757 , -1.6163229 ],\n",
       "       [-2.033528  ,  1.9582025 ],\n",
       "       [-2.550745  ,  2.576912  ],\n",
       "       [ 1.6066593 , -1.4532995 ],\n",
       "       [-0.6680107 ,  0.6142806 ],\n",
       "       [ 1.3431389 , -1.2320627 ],\n",
       "       [ 0.8650277 , -0.83068097],\n",
       "       [ 1.326538  , -1.2327954 ],\n",
       "       [-1.3006853 ,  1.3430374 ],\n",
       "       [-1.9171124 ,  1.8675165 ],\n",
       "       [ 0.8855611 , -0.70056236]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "model = torch.load('./models/multi_model')\n",
    "result, model_outputs, wrong_predictions = model.eval_model(df_bn_test, f1=sklearn.metrics.f1_score, acc=sklearn.metrics.accuracy_score)\n",
    "model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n"
     ]
    }
   ],
   "source": [
    "print(len(en_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "feat_embeds = []\n",
    "for key, value in wt_dict.items():\n",
    "    for idx, num in enumerate(value['indices']):\n",
    "        if num < len(en_embeds):#trick to get indices you want\n",
    "            feat_embeds.append(softmax(value['outputs'][idx]))\n",
    "feat_embeds = np.asarray(feat_embeds)\n",
    "feat_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92410326, 0.07589663],\n",
       "       [0.9467504 , 0.05324958],\n",
       "       [0.9671815 , 0.03281866],\n",
       "       ...,\n",
       "       [0.25433308, 0.7456669 ],\n",
       "       [0.02572706, 0.9742729 ],\n",
       "       [0.98565495, 0.01434495]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1333, 2)\n"
     ]
    }
   ],
   "source": [
    "#only for multilingual or bengali\n",
    "model_outputs = np.asarray([softmax(wt) for wt in model_outputs])\n",
    "feat_embeds = np.append(feat_embeds, model_outputs, 0)\n",
    "print(feat_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#link scores\n",
    "en_scr = pd.read_csv('./resources/en_score.csv')\n",
    "en_scr = en_scr['score'].to_list()\n",
    "bn_scr = pd.read_csv('./resources/bn_score.csv')\n",
    "bn_scr = bn_scr['score'].to_list()\n",
    "hi_scr = pd.read_csv('./resources/hn_score1.csv')\n",
    "hi_scr = hi_scr['score'].to_list()\n",
    "bn_scr_2 = bn_scr[-100:] \n",
    "bn_scr = bn_scr[:len(bn_scr)-100]\n",
    "#link_scr = en_scr + bn_scr + hi_scr + bn_scr_2\n",
    "link_scr = en_scr\n",
    "len(link_scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/profanity_scores.pickle', 'rb') as pkl_in:\n",
    "    pscores = pkl.load(pkl_in)\n",
    "pscores = pscores[:len(en_embeds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 3)\n"
     ]
    }
   ],
   "source": [
    "#, text_ft_en[i][0], text_ft_en[i][1], text_ft_en[i][2], text_ft_en[i][3], text_ft_en[i][4]])\n",
    "#concat features\n",
    "ft_emb = []\n",
    "#score,\n",
    "#from sklearn import preprocessing\n",
    "for i, score in enumerate(pscores):\n",
    "    ft_emb.append([link_scr[i], feat_embeds[i][0], feat_embeds[i][1]])\n",
    "ft_emb = np.asarray(ft_emb)\n",
    "print(ft_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02273516, 0.92410326, 0.07589663])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/covid_embeddings_en.pickle', 'wb') as pkl_out:\n",
    "    pkl.dump(ft_emb, pkl_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/features.pickle', 'wb') as pkl_out:\n",
    "    pkl.dump(feat_embeds, pkl_out)\n",
    "    pkl.dump(pscores, pkl_out)\n",
    "    pkl.dump(link_scr, pkl_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = {word : i + 1 for i, word in enumerate(words)}\n",
    "with open('./resources/multi_word_idx_en.pickle', 'wb') as pkl_out:\n",
    "    pkl.dump(word_idx, pkl_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_idx = {sent : i + 1 for i, sent in enumerate(train_data + eval_data + test_data)}\n",
    "with open('./resources/multi_sent_idx_en.pickle', 'wb') as pkl_out:\n",
    "    pkl.dump(sent_idx, pkl_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covidfake",
   "language": "python",
   "name": "covidfake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
