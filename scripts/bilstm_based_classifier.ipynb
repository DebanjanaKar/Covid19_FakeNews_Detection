{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#@author : Debanjana\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "manualSeed = 1525\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, X2, y, batch_size=64, return_idx=False):\n",
    "    \n",
    "    for offset in range(0, X.shape[0], batch_size):\n",
    "        #print(len(X), X.shape, X2.shape)\n",
    "        #print('---------------------------------------------------')\n",
    "        #print('offset', offset)\n",
    "        y = np.asarray(y)\n",
    "        batch_X2_len=X2[offset:offset+batch_size]!=0\n",
    "        batch_X_len=np.sum(X[offset:offset+batch_size]!=0, axis=1)\n",
    "        batch_idx2=batch_X2_len.argsort()\n",
    "        batch_idx=batch_X_len.argsort()[::-1]\n",
    "        #print(batch_idx, batch) #[::-1]\n",
    "        batch_X_len=batch_X_len[batch_idx]\n",
    "        batch_X2_len=batch_X2_len[batch_idx2]\n",
    "        batch_X_mask=(X[offset:offset+batch_size]!=0)[batch_idx].astype(np.uint8)\n",
    "        batch_X=X[offset:offset+batch_size][batch_idx]\n",
    "        batch_X2=X2[offset:offset+batch_size][batch_idx2]\n",
    "        batch_y=y[offset:offset+batch_size][batch_idx]\n",
    "        batch_X = torch.autograd.Variable(torch.from_numpy(batch_X).long() )\n",
    "        batch_X2 = torch.autograd.Variable(torch.from_numpy(batch_X2).long() )\n",
    "        batch_X_mask=torch.autograd.Variable(torch.from_numpy(batch_X_mask).long() )\n",
    "        batch_y = torch.autograd.Variable(torch.from_numpy(batch_y).long() )\n",
    "        #print(batch_X, batch_y)\n",
    "        if len(batch_y.size() )==2 :\n",
    "            try:\n",
    "                batch_y=torch.nn.utils.rnn.pack_padded_sequence(batch_y, batch_X2_len, batch_first=True)\n",
    "            except:\n",
    "                continue\n",
    "        if return_idx: \n",
    "            #in testing, need to sort back.\n",
    "            yield (batch_X, batch_y, batch_X_len, batch_X_mask, batch_idx)\n",
    "        else:\n",
    "            yield (batch_X, batch_X2, batch_y, batch_X2_len, batch_X_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeder(manualSeed):\n",
    "    np.random.seed(manualSeed)\n",
    "    random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "    \n",
    "    torch.backends.cudnn.enabled = False \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, ft_emb, tweet_emb, num_classes=2, dropout=0.5):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        manualSeed = 1525\n",
    "        #print('shapes : ',gen_emb.shape[0], gen_emb.shape[1] )\n",
    "        seeder(manualSeed)\n",
    "        \n",
    "        self.ft_embedding = torch.nn.Embedding(ft_emb.shape[0], ft_emb.shape[1]) #no. of emb x emb dim\n",
    "        \n",
    "        self.ft_embedding.weight=torch.nn.Parameter(torch.from_numpy(ft_emb), requires_grad=False)\n",
    "        \n",
    "        self.tweet_embedding = torch.nn.Embedding(tweet_emb.shape[0], tweet_emb.shape[1]) #no. of emb x emb dim\n",
    "        \n",
    "        self.tweet_embedding.weight=torch.nn.Parameter(torch.from_numpy(tweet_emb), requires_grad=False)\n",
    "                \n",
    "        seeder(manualSeed)\n",
    "        \n",
    "        self.linear_features=torch.nn.Linear(ft_emb.shape[1], ft_emb.shape[1] ) \n",
    "        \n",
    "        seeder(manualSeed)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size = tweet_emb.shape[1], hidden_size = 128, num_layers = 1, bidirectional = True)\n",
    "        \n",
    "        seeder(manualSeed)\n",
    "                \n",
    "        self.dropout=torch.nn.Dropout(dropout)\n",
    "        \n",
    "        seeder(manualSeed)\n",
    "        \n",
    "        self.linear_final=torch.nn.Linear(256 + ft_emb.shape[1], num_classes) \n",
    "        \n",
    "            \n",
    "    def forward(self, x, x2,  x_len, x_mask, x_tag=None, testing=False):\n",
    "        #print('gen',self.gen_embedding(x).shape, type(self.gen_embedding(x)))\n",
    "        seeder(manualSeed)\n",
    "        \n",
    "        ft_emb = self.ft_embedding(x2).type(torch.FloatTensor) #ft = link score + bias + class_wts\n",
    "        \n",
    "        ft_emb = torch.nn.functional.relu(self.linear_features(ft_emb))\n",
    "        \n",
    "        twt_emb = self.tweet_embedding(x).type(torch.FloatTensor)\n",
    "        \n",
    "        #print(ft_emb.shape, twt_emb.shape)\n",
    "        \n",
    "        seeder(manualSeed)\n",
    "        \n",
    "        ft_emb = self.linear_features(ft_emb)\n",
    "        \n",
    "        seeder(manualSeed)\n",
    "        \n",
    "        twt_emb, _ = self.lstm(twt_emb)\n",
    "        \n",
    "        twt_emb = torch.nn.functional.relu(twt_emb)\n",
    "        \n",
    "        pool = torch.nn.MaxPool1d(twt_emb.shape[1], stride=1)\n",
    "        \n",
    "        twt_emb = pool(twt_emb.transpose(1,2))\n",
    "        #print(ft_emb.shape, twt_emb.shape)\n",
    "        \n",
    "        twt_emb = self.dropout(twt_emb)\n",
    "        ft_emb.unsqueeze_(-1)\n",
    "        ft_emb = ft_emb.expand(ft_emb.shape[0],ft_emb.shape[1],1)\n",
    "        \n",
    "        #print(ft_emb.shape, twt_emb.shape)\n",
    "        \n",
    "        emb = torch.cat((ft_emb, twt_emb), dim=1).transpose(1,2)\n",
    "        \n",
    "        #print(emb.shape)\n",
    "        \n",
    "        seeder(manualSeed)\n",
    "        \n",
    "        x_logit=self.linear_final(emb)\n",
    "        \n",
    "        #print(x_logit.shape)\n",
    "        \n",
    "        seeder(manualSeed)\n",
    "\n",
    "        if testing:\n",
    "            x_logit=x_logit.transpose(2, 0)\n",
    "            score=torch.nn.functional.log_softmax(x_logit).transpose(2, 0)\n",
    "        else:\n",
    "            x_logit=torch.nn.utils.rnn.pack_padded_sequence(x_logit, x_len, batch_first=True)   #removes padding\n",
    "            score=torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(x_logit.data), x_tag.data)\n",
    "        \n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loss(model, valid_X, valid_X2, valid_y):\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    for batch in batch_generator(valid_X, valid_X2, valid_y):\n",
    "        batch_valid_X, batch_valid_X2, batch_valid_y, batch_valid_X_len, batch_valid_X_mask=batch\n",
    "        loss=model(batch_valid_X, batch_valid_X2, batch_valid_X_len, batch_valid_X_mask, batch_valid_y)\n",
    "        #print(loss)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_X, train_X2, train_y, valid_X, valid_X2, valid_y, model, model_fn, optimizer, parameters, epochs=200, batch_size=64):\n",
    "    best_loss=float(\"inf\") \n",
    "    valid_history=[]\n",
    "    train_history=[]\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for batch in batch_generator(train_X, train_X2, train_y, batch_size):\n",
    "            batch_train_X, batch_train_X2, batch_train_y, batch_train_X_len, batch_train_X_mask=batch\n",
    "            loss=model(batch_train_X, batch_train_X2, batch_train_X_len, batch_train_X_mask, batch_train_y) #going to forward directly by matching parameters?\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(parameters, 1.)\n",
    "            optimizer.step()\n",
    "        loss=valid_loss(model, train_X, train_X2, train_y)\n",
    "        train_history.append(loss)\n",
    "        print('epoch ', epoch, ' trained..loss = ',loss )\n",
    "        loss=valid_loss(model, valid_X, valid_X2, valid_y)\n",
    "        print('epoch ', epoch, ' validated..loss = ',loss )\n",
    "        valid_history.append(loss)        \n",
    "        if loss<best_loss:\n",
    "            best_loss=loss\n",
    "            torch.save(model, model_fn)\n",
    "        shuffle_idx=np.random.permutation(len(train_X) )\n",
    "        train_X=train_X[shuffle_idx]\n",
    "        train_X2=train_X2[shuffle_idx]\n",
    "        train_y=train_y[shuffle_idx]\n",
    "    model=torch.load(model_fn) \n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_X, test_X2, batch_size=64, crf=False):\n",
    "    \n",
    "    pred_y=np.zeros((test_X.shape[0], 1), np.int16)\n",
    "    model.eval()\n",
    "    \n",
    "    for offset in range(0, test_X.shape[0], batch_size):\n",
    "        \n",
    "        batch_test_X_len=np.sum(test_X[offset:offset+batch_size]!=0, axis=1)\n",
    "        batch_test_X2_len=test_X2[offset:offset+batch_size]!=0\n",
    "        batch_idx=batch_test_X_len.argsort()[::-1]\n",
    "        batch_idx2=batch_test_X2_len.argsort()\n",
    "        batch_test_X_len=batch_test_X_len[batch_idx]\n",
    "        batch_test_X2_len=batch_test_X2_len[batch_idx2]\n",
    "        batch_test_X_mask=(test_X[offset:offset+batch_size]!=0)[batch_idx].astype(np.uint8)\n",
    "        batch_test_X=test_X[offset:offset+batch_size][batch_idx]\n",
    "        batch_test_X2=test_X2[offset:offset+batch_size][batch_idx2]\n",
    "        batch_test_X_mask=torch.autograd.Variable(torch.from_numpy(batch_test_X_mask).long() )\n",
    "        batch_test_X = torch.autograd.Variable(torch.from_numpy(batch_test_X).long() )\n",
    "        batch_test_X2 = torch.autograd.Variable(torch.from_numpy(batch_test_X2).long() )\n",
    "        batch_pred_y=model(batch_test_X, batch_test_X2, batch_test_X_len, batch_test_X_mask, testing=True)\n",
    "        r_idx=batch_idx.argsort()\n",
    "        batch_pred_y=batch_pred_y.data.cpu().numpy().argmax(axis=2)[r_idx]\n",
    "        pred_y[offset:offset+batch_size,:batch_pred_y.shape[1]]=batch_pred_y\n",
    "    \n",
    "    model.train()\n",
    "    assert len(pred_y)==len(test_X)\n",
    "    \n",
    "    return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_formatter(tweets, train_idx, sent_idx, corpus_len=0):\n",
    "    \n",
    "    sent_list1 = []\n",
    "    for i, sent in enumerate(tweets):\n",
    "        sents = [train_idx[word + '_' + str(i +corpus_len)] for word in sent.split()]\n",
    "        sent_list1.append(sents)\n",
    "    \n",
    "    #padding\n",
    "    MAX = max([len(sent) for sent in sent_list1])\n",
    "    print('Max length of tweet = ', MAX)\n",
    "    \n",
    "    for i in range(len(sent_list1)):\n",
    "        words = np.zeros(MAX)\n",
    "        words[:len(sent_list1[i])] = sent_list1[i]\n",
    "        sent_list1[i] = words\n",
    "            \n",
    "                    \n",
    "    sent_list2 = [sent_idx[sent] for i, sent in enumerate(tweets)] \n",
    "    \n",
    "    print(np.array(sent_list1).shape, np.array(sent_list2).shape)\n",
    "    \n",
    "    return np.array(sent_list1), np.array(sent_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_dir, model_dir, runs, epochs, lr, dropout, batch_size=128):\n",
    "    \n",
    "    with open(data_dir+\"covid_embeddings_en.pickle\", \"rb\") as pkl_in:\n",
    "        feat_emb = pkl.load(pkl_in)      #link score + class wt + bias ft\n",
    "        \n",
    "    with open('./resources/tweet_word_emb_en.pickle', 'rb') as pkl_in:\n",
    "        tweet_word_emb = pkl.load(pkl_in)\n",
    "    \n",
    "    print(feat_emb.shape)\n",
    "    \n",
    "    feat_emb = np.insert(feat_emb, 0, np.ones(len(feat_emb[0])), axis = 0)  #inserting embedding for pad\n",
    "    tweet_word_emb = np.insert(tweet_word_emb, 0, np.ones(len(tweet_word_emb[0])), axis = 0) #inserting embedding for pad\n",
    "    \n",
    "    with open('./resources/tweets_preprocessed_multi.pickle', 'rb') as pkl_in:\n",
    "        train_data = pkl.load(pkl_in)\n",
    "        valid_data = pkl.load(pkl_in)\n",
    "        test_data = pkl.load(pkl_in) \n",
    "\n",
    "        train_tags = pkl.load(pkl_in)\n",
    "        valid_tags = pkl.load(pkl_in)\n",
    "        test_tags = pkl.load(pkl_in)\n",
    "        \n",
    "    with open(data_dir+\"multi_word_idx_en.pickle\", \"rb\") as pkl_in:\n",
    "        word_idx = pkl.load(pkl_in)\n",
    "        \n",
    "    with open(data_dir+\"multi_sent_idx_en.pickle\", \"rb\") as pkl_in:\n",
    "        sent_idx = pkl.load(pkl_in)\n",
    "\n",
    "   \n",
    "    print('test data sample : ', test_data[0])\n",
    "    print('Length of train and test data : ', len(train_data), len(test_data))\n",
    "    print('Length of train and test tags : ', len(train_tags), len(test_tags))\n",
    "    print('--------------------------------------------------------------------')\n",
    "    \n",
    "    test_X, test_y = test_data, np.asarray(test_tags)\n",
    "    train_X, train_y = train_data, np.asarray(train_tags)\n",
    "    valid_X_true, valid_y = valid_data, np.asarray(valid_tags)\n",
    "    \n",
    "\n",
    "    print('valid data sample : ', valid_X_true[0])\n",
    "    \n",
    "    valid_X_1, valid_X_2 = data_formatter(valid_X_true, word_idx, sent_idx, len(train_X))\n",
    "    train_X_1, train_X_2 = data_formatter(train_X, word_idx, sent_idx, 0)\n",
    "    test_X_1, test_X_2 = data_formatter(test_X, word_idx, sent_idx, len(train_data)+len(valid_X_true))\n",
    "    \n",
    "    \n",
    "    for r in range(runs):\n",
    "        print('-------------------------------------------------------------------------------')\n",
    "        print(r)\n",
    "        \n",
    "        model=Model(feat_emb, tweet_word_emb, num_classes=2, dropout=dropout)\n",
    "        #model.cuda()\n",
    "        \n",
    "        parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer=torch.optim.Adam(parameters, lr=lr)\n",
    "        train_history, valid_history=train(train_X_1, train_X_2, train_y, valid_X_1, valid_X_2, valid_y, model, model_dir+str(r), optimizer, parameters, epochs, batch_size)\n",
    "        \n",
    "        model=torch.load(model_dir+str(r))\n",
    "        pred=test(model, test_X_1, test_X_2, batch_size, crf=False)\n",
    "    \n",
    "        print(precision_recall_fscore_support(test_y, pred, average='macro'))\n",
    "        print(precision_recall_fscore_support(test_y, pred, average='micro'))\n",
    "        print(precision_recall_fscore_support(test_y, pred, average='weighted'))\n",
    "\n",
    "    return train_y, test_y, train_history, valid_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing code\n",
      "(504, 3)\n",
      "test data sample :  a state of emergency has been declared for la city and county due to confirmed cases of coronavirus . angelenos need to be prepared , but not panicked . listen to the advice of experts . make sure you ' re following @cdcgov and @la public health for the latest info and recommendations . <link>\n",
      "Length of train and test data :  403 51\n",
      "Length of train and test tags :  403 51\n",
      "--------------------------------------------------------------------\n",
      "valid data sample :  safety always comes first . do not panic and stay safe .  corona alert <link>\n",
      "Max length of tweet =  71\n",
      "(50, 71) (50,)\n",
      "Max length of tweet =  125\n",
      "(403, 125) (403,)\n",
      "Max length of tweet =  73\n",
      "(51, 73) (51,)\n",
      "-------------------------------------------------------------------------------\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debanjana_ibm/anaconda3/envs/covidfake/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/debanjana_ibm/anaconda3/envs/covidfake/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  trained..loss =  0.6469668320247105\n",
      "epoch  0  validated..loss =  0.642158567905426\n",
      "epoch  1  trained..loss =  0.6445313010896955\n",
      "epoch  1  validated..loss =  0.6244661211967468\n",
      "epoch  2  trained..loss =  0.6182243142809186\n",
      "epoch  2  validated..loss =  0.6156518459320068\n",
      "epoch  3  trained..loss =  0.6116836581911359\n",
      "epoch  3  validated..loss =  0.6071416139602661\n",
      "epoch  4  trained..loss =  0.6084710870470319\n",
      "epoch  4  validated..loss =  0.5993238687515259\n",
      "epoch  5  trained..loss =  0.5909407649721418\n",
      "epoch  5  validated..loss =  0.5924621820449829\n",
      "epoch  6  trained..loss =  0.5794044733047485\n",
      "epoch  6  validated..loss =  0.588848352432251\n",
      "epoch  7  trained..loss =  0.5758888295718602\n",
      "epoch  7  validated..loss =  0.5809667110443115\n",
      "epoch  8  trained..loss =  0.5555441294397626\n",
      "epoch  8  validated..loss =  0.5718463659286499\n",
      "epoch  9  trained..loss =  0.5508480497768947\n",
      "epoch  9  validated..loss =  0.5651542544364929\n",
      "epoch  10  trained..loss =  0.5350904975618634\n",
      "epoch  10  validated..loss =  0.5589669942855835\n",
      "epoch  11  trained..loss =  0.533370988709586\n",
      "epoch  11  validated..loss =  0.5539220571517944\n",
      "epoch  12  trained..loss =  0.5104046208517892\n",
      "epoch  12  validated..loss =  0.5459982752799988\n",
      "epoch  13  trained..loss =  0.5019328253609794\n",
      "epoch  13  validated..loss =  0.5410502552986145\n",
      "epoch  14  trained..loss =  0.5021251269749233\n",
      "epoch  14  validated..loss =  0.5334782600402832\n",
      "epoch  15  trained..loss =  0.4858183647905077\n",
      "epoch  15  validated..loss =  0.5259103775024414\n",
      "epoch  16  trained..loss =  0.47589926634516033\n",
      "epoch  16  validated..loss =  0.5213577151298523\n",
      "epoch  17  trained..loss =  0.4777844377926418\n",
      "epoch  17  validated..loss =  0.5137824416160583\n",
      "epoch  18  trained..loss =  0.45942140902791706\n",
      "epoch  18  validated..loss =  0.508719265460968\n",
      "epoch  19  trained..loss =  0.44935134053230286\n",
      "epoch  19  validated..loss =  0.5016845464706421\n",
      "epoch  20  trained..loss =  0.4371629314763205\n",
      "epoch  20  validated..loss =  0.49564918875694275\n",
      "epoch  21  trained..loss =  0.423763313463756\n",
      "epoch  21  validated..loss =  0.48797616362571716\n",
      "epoch  22  trained..loss =  0.4186029391629355\n",
      "epoch  22  validated..loss =  0.4846464991569519\n",
      "epoch  23  trained..loss =  0.4051927924156189\n",
      "epoch  23  validated..loss =  0.47898852825164795\n",
      "epoch  24  trained..loss =  0.39416017276900156\n",
      "epoch  24  validated..loss =  0.47183236479759216\n",
      "epoch  25  trained..loss =  0.3842077340398516\n",
      "epoch  25  validated..loss =  0.4678240120410919\n",
      "epoch  26  trained..loss =  0.37787051286016193\n",
      "epoch  26  validated..loss =  0.46304798126220703\n",
      "epoch  27  trained..loss =  0.35946578213146757\n",
      "epoch  27  validated..loss =  0.46112874150276184\n",
      "epoch  28  trained..loss =  0.3554935029574803\n",
      "epoch  28  validated..loss =  0.45375946164131165\n",
      "epoch  29  trained..loss =  0.34844518985067097\n",
      "epoch  29  validated..loss =  0.451750785112381\n",
      "epoch  30  trained..loss =  0.3350760681288583\n",
      "epoch  30  validated..loss =  0.44525882601737976\n",
      "epoch  31  trained..loss =  0.31530582904815674\n",
      "epoch  31  validated..loss =  0.4438144266605377\n",
      "epoch  32  trained..loss =  0.30786831038338797\n",
      "epoch  32  validated..loss =  0.44027331471443176\n",
      "epoch  33  trained..loss =  0.29682627746037077\n",
      "epoch  33  validated..loss =  0.43716952204704285\n",
      "epoch  34  trained..loss =  0.2927455348627908\n",
      "epoch  34  validated..loss =  0.42987456917762756\n",
      "epoch  35  trained..loss =  0.2974820498909269\n",
      "epoch  35  validated..loss =  0.42729130387306213\n",
      "epoch  36  trained..loss =  0.28590733664376394\n",
      "epoch  36  validated..loss =  0.43069732189178467\n",
      "epoch  37  trained..loss =  0.2647844744580133\n",
      "epoch  37  validated..loss =  0.4195444583892822\n",
      "epoch  38  trained..loss =  0.25619131326675415\n",
      "epoch  38  validated..loss =  0.41549819707870483\n",
      "epoch  39  trained..loss =  0.24483731176171983\n",
      "epoch  39  validated..loss =  0.41573578119277954\n",
      "epoch  40  trained..loss =  0.2406539704118456\n",
      "epoch  40  validated..loss =  0.4185037314891815\n",
      "epoch  41  trained..loss =  0.2368128321000508\n",
      "epoch  41  validated..loss =  0.40887507796287537\n",
      "epoch  42  trained..loss =  0.22219406919819967\n",
      "epoch  42  validated..loss =  0.40653514862060547\n",
      "epoch  43  trained..loss =  0.21561898503984725\n",
      "epoch  43  validated..loss =  0.4082816243171692\n",
      "epoch  44  trained..loss =  0.19887748999255045\n",
      "epoch  44  validated..loss =  0.4038914740085602\n",
      "epoch  45  trained..loss =  0.19333585671016149\n",
      "epoch  45  validated..loss =  0.4066154360771179\n",
      "epoch  46  trained..loss =  0.19166105772767747\n",
      "epoch  46  validated..loss =  0.39885374903678894\n",
      "epoch  47  trained..loss =  0.1842218531029565\n",
      "epoch  47  validated..loss =  0.39769795536994934\n",
      "epoch  48  trained..loss =  0.17678144787039077\n",
      "epoch  48  validated..loss =  0.39649173617362976\n",
      "epoch  49  trained..loss =  0.17149871587753296\n",
      "epoch  49  validated..loss =  0.3947769105434418\n",
      "(0.7625482625482626, 0.7352941176470589, 0.745115856428896, None)\n",
      "(0.7843137254901961, 0.7843137254901961, 0.7843137254901961, None)\n",
      "(0.7786357786357786, 0.7843137254901961, 0.7784340451310011, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debanjana_ibm/anaconda3/envs/covidfake/lib/python3.6/site-packages/ipykernel_launcher.py:81: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print('testing code')\n",
    "    train_y, true, th, vh = run('./resources/','./resources/model_meta', 1, 50, 0.0001, 0.4, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.8168981481481481, 0.8134157509157509, 0.8149597238204833, None)\n",
    "(0.8208955223880597, 0.8208955223880597, 0.8208955223880597, None)\n",
    "(0.8202805417357657, 0.8208955223880597, 0.8204008725074283, None)\n",
    "English\n",
    "(0.7580357142857144, 0.75, 0.7536231884057971, None)\n",
    "(0.7843137254901961, 0.7843137254901961, 0.7843137254901961, None)\n",
    "(0.7815476190476192, 0.7843137254901961, 0.7826086956521741, None)\n",
    "English with text fts\n",
    "(0.7352941176470589, 0.7352941176470589, 0.7352941176470589, None)\n",
    "(0.7647058823529411, 0.7647058823529411, 0.7647058823529412, None)\n",
    "(0.7647058823529411, 0.7647058823529411, 0.7647058823529411, None)\n",
    "w/o link\n",
    "(0.7361111111111112, 0.7205882352941176, 0.7267857142857144, None)\n",
    "(0.7647058823529411, 0.7647058823529411, 0.7647058823529412, None)\n",
    "(0.7592592592592592, 0.7647058823529411, 0.7607142857142858, None)\n",
    "w/o bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3RVVdrH8e9OrySkV0pCgFCS0EJHxAKCoqLSVBRkEOvgjOOgzjszOmN3HHUUBRVUBDsqioKKoAhSQk8IEFpIgYQkpJKe/f5xokYMEEhyz703z2etrNtOzn3OuPhlzz67KK01QgghbJ+D2QUIIYRoGRLoQghhJyTQhRDCTkigCyGEnZBAF0IIO+Fk1hcHBAToTp06mfX1Qghhk7Zu3ZqntQ5s7DPTAr1Tp04kJSWZ9fVCCGGTlFLpZ/pMulyEEMJOSKALIYSdkEAXQgg7IYEuhBB2QgJdCCHshAS6EELYCQl0IYSwE6aNQ79QB3JLWb4zmy5BXsQEedE5wBM3Z0ezyxJCCNPZXKCnHivmpe/SqKtfxt1BQUd/T6IDvYgN9WbmsCh8PJzNLVIIIUxgc4F+VbQTVwz7nrT4BziQX0VabikHcks4kFvKd3tzyCos57mJCWaXKYQQFmdzgU76Bpy2zCe2pozY8S+BUr989NTKvbyy9iC3DulEXISviUUKIYTl2d5N0Z7XwEV/he3vwPoXfvPRnSOjCfBy4dHP9yBb6wkh2hrbC3SAkQ9Czwnw7T8h9fNf3vZ2c+b+y7uRlH6SL3cfN68+IYQwgW0GulJwzTwI7wfLZkH2jl8+uqF/JLGh7Xjiq1QqqmtNLFIIISzLNgMdwNkdJi8FD394dzIUZwPg6KD4v3GxZJ4sZ+H6wyYXKYQQlmO7gQ7gHQxT3oPKEiPUq8oAGNIlgEtjg3n5uwPkllSYXKQQQliGbQc6QEgvuH4hHN9tdL/U1QHw8LhYqmrreO7r/SYXKIQQlmH7gQ7QdTRc/hjs/QLeHAvHdtE5wJNpgzvxflIGKdlFZlcohBCtzj4CHWDQHTD+JcjbDwsugi/+xB8HB+Dr7sy/v0iVYYxCCLtnP4GuFPS9Ge7ZCgP+AFsX0e71gczrvpNNh07w5oYjVNfWmV2lEEK0GmVWy7V///66VTeJzkmBLx+A9B856NSFB8puJLtdPNMGd2JKYiS+Hi6t991CCNFKlFJbtdb9G/3MbgMdQGtIWYZe9TdUSTY/uY/gL4XXkecUzHV9I5g+tDNdgrxatwYhhGhBbTfQf1ZVButfhPUvUKfrWOM3kfuPjeJkjSv9OranbwdfEiLbk9DBlzAfN1SD9WGEEMKaSKD/rCgTvn0Edn9AnWcQ34XN5pXCRHYfK6WqxuhfD/R2JSHSl4u7BTElMVLCXQhhVc4W6La32mJz+ETAda9B4iwcVs7l0rRHuTS4N9U3PswezyHsyCxiR0Yh246e5Js9ORSVV3PHyGizqxZCiCZp0igXpdQYpdQ+pdQBpdTcMxwzUim1QymVopT6vmXLbGGRA2DmtzDhdagqwfn9ycR/M5lbwrL476QE1t4/kqviw3hq5V6+3H3M7GqFEKJJzhnoSilH4GXgCqAHMEUp1eO0Y3yBecB4rXVP4IZWqLVlKQVxN8BdW2Dcc1CYbkxKWjwBdWwHz1wfR98Ovtz3/g52ZBSaXa0QQpxTU1roicABrfUhrXUV8B5w9WnHTAWWaa2PAmitc1u2zFbk5AIDboN7t8Pl/4bs7bBgJG4fT2PhZY4EtXNl5ltJZBWWm12pEEKcVVMCPRzIaPA6s/69hroC7ZVSa5VSW5VS0xo7kVJqllIqSSmVdOLEiQuruLU4u8OQe+CPO4311o+sw3fJaFb5PkVizRZmLNxMSUW12VUKIcQZNSXQGxvmcfrQGCegHzAOGA38n1Kq6+9+SesFWuv+Wuv+gYGB512sRbi1g5Fz4b4UGP04HqUZzONJ/ld4J+8ueIqaKlm9UQhhnZoS6JlAZIPXEUB2I8es1FqXaa3zgB+A+JYp0SSu3jD4LqPFfu18gnzcmVXwDKee7oH+9A7YthjyDxqTl4QQwgqccxy6UsoJ2A9cAmQBW4CpWuuUBsfEAi9htM5dgM3AZK118pnOa8o49ObQmvfeXYRP6lJGuOzHs7Z+BUevEOg4xPjpdR14+JlbpxDCrjVrHLrWukYpdTewCnAEFmqtU5RSs+s/f1VrnaqUWgnsAuqA188W5jZJKSZOns7zq4dx5+p9XBZYxBP9SvDPS4L0DZCyDFY/avTDD7oTXGVJASGEZbWtmaItZM2+XO57fwe1tZr/TIzn8h7BxmJga58w1mT3CIAR90P/GeDkana5Qgg7crYWuv0sn2tBF3cL4vO7h9E50JNZi7fy5Mp91AT2gMlLYOZqCO4BK+fC//rB9iVQJ5tVCyFanwT6BYr08+DD2YO5cWAHXv3+IDe9sYmNh/KpCO4Dt3wON38KngHw2Z0wfwQc3WR2yUIIOyddLi1g2bZMHv4kmfLqWpwdFb3CfRjQyY/+HXwZXPUj3mv/AcVZ0OdmuPQR8PQ3u2QhhI2S1RYtoKi8mq3pBWw5cpIthwvYlVlEVf0OSbMGBvGQ53LYOM8YDnnpI0a4O8j/QRJCnB8JdBNUVNeyO6uI9zZn8PG2TF6YnMDVoUWw4s9wdANEJMLYZyAswexShRA2RG6KmsDN2ZEBnfx46rre9O/Ynoc/SeaIY0eY/iVc8woUHDI2s377Gkj7ViYoCSGaTQK9lTk5OvDClD44KLjn3e1U1WpImGpsZn3J3yE3FZZcB/MGG7NPq2VpASHEhZFAt4BwX3eevj6e3VlFPL1yr/Gmuy8M/zPM2Q3XvAoOjrD8bni+F6x9CkpyzC1aCGFzJNAtZEyvEKYN7sjrPx7mu70NwtrJBRKmwOwfYdpnEJoAax+H//aAD6fDkfXSHSOEaBK5KWpBFdW1XPPyenJLKvny3uGE+Lg1fmDeAUhaCDvegYoiCIw11myPm2SsBimEaLPkpqiVcHN25KWpfSmvqmXO+9uprTvDH9OALjDmcfjTXhj/ktGK//J+eC4WPv4D7PsKaiotW7wQwupJC90EHyZl8JePdnHL4I7cMbLLmVvqP9MasrbB1kWQ+jlUFIKrD3QfBz2vhaiRRugLIeyejEO3Mlpr5n68m/eTjI2g4iN9ubxHMKN7htAl6ByrNNZWw6G1kPIJpH4BlUXg5mssBDb0j8bNViGE3ZJAt0Jaaw6eKGVVSg5fpxxnZ6axvnpUoCdXx4dz18XRODmeo0esphIOroGdS2HPZ0awD7sPEmeBi4cFrkIIYWkS6DYgu7Ccb1NzWJl8nA0H85nQJ5xnb4jHwaGxHQAbcWwXfPcvSPva2HRj5F+N5QUcnVu3cCGERUmg25gXV6fx3Df7uWlQB/51dS+UamKog7HZxrePQMZG8IuCIfdC7HhZEEwIOyGjXGzMPaO6MGtEFO9sPMqTK/dyXn90Ow6BGSthyvvg7AFfzIFnY+DtqyFpEZTltV7hQghTnXMLOmF5SikevKI7ZZU1zP/+EN6uTtw9KuZ8TgDdxkDX0XB8F6R8Cns+NcJ9xZ+h0zCImwi9J8roGCHsiHS5WLG6Os2fP9zJJ9uz+MdVPZg+tPOFn0xryEn+NdzzD4BvBxjxAMRPlr52IWyEdLnYKAcHxTPXxzG6ZzCPfL6HD7ZkXPjJlIKQ3nDJ/8HdSTD1Q/DwN9aPeWkA7HgXamtarnghhMVJC90GVNbUMvOtJNal5RHSzo0wXzdCfd0J93Un1MeNcF93LuoWiKuT4/mdWGvYvxLWPG50zfhFw8i50Os6Y7EwIYTVkVEudqC8qpZFGw5z6EQZx4rKyS6sILuwnMoaY1ekcXGhvDy174WdXGvYuwLWPmF0ywR0NYK9x7Wyq5IQVkYC3U5prSkoq2Lh+sO8vOYg82/ux+ieIRd+wro6SF0Oa5+EE6nGomAj5xrDHiXYhbAK0odup5RS+Hu5MufSrnQP8eb/Pk2mqLz6wk/o4AA9r4E7NsD1C0HXwoe3wPzhxhoy0scuhFWTQLcDzo4OPHN9PPllVTy+IrX5J3RwMPrR79wIE16D6nJ4/yZjPPuy242RMpUlzf8eIUSLknHodqJ3hA9/GB7Fq98fZHxCGEO7BDT/pA6Oxnj1nhNg35dGP3vaKtj1Hji6QKfh0O0KI/w9/Jr/fUKIZpE+dDtSUV3L2BfWUV1Xx6o5I/BwaYW/17U1kLHJCPh9XxqbXbt4wcDZMPguCXYhWpn0obcRbs6OPHldHBkF5Ty7an/rfImjE3QaCqMfg3u2GVvnxVwG656FF+JhzRNQXtg63y2EOCsJdDuT2NmPaYM7smjDYbamn2zdL/t5stINbxo3UqNGwvdPwgtx8P3TxvZ5QgiLkUC3Qw+M6U6Yjzt//XgXlTW1lvnS4J4waTHcvg46DoM1j8GzXY2bqckfQ1WZZeoQog2TQLdDXq5OPD6hNwdyS3n4k2SKK5oxlPF8hcbBlKVw+w/Q9xbI2AwfzYBnusCHt8Ke5caoGSFEi5ObonbsqZV7eWXtQfw9XfjT5V2Z1D/y3LsgtbS6WmON9pRPjF2VTuWBm4+xq9LA2eDZAqNxhGhDmj1TVCk1BngBcARe11o/edrnI4HPgMP1by3TWj96tnNKoFvG7swi/vXFHjYfKaBbsDd/uzKW4TGB5hRTWwNH1kHSG8Z+qE5u0HcaDLkHfCPNqUkIG9OsQFdKOQL7gcuATGALMEVrvafBMSOB+7XWVza1KAl0y9FaszL5OI9/lUpGQTmXdA/ituGdcXF0oLZOU6u18Vin8fN0IS7CAhtNn9gP618wxrQD9L4Bhs6BoO6t/91C2LDmBvpg4J9a69H1rx8E0Fo/0eCYkUigW73KmlreXH+El747QEnlmafx//3KHswY1oy1189HUSZseAm2vQXVp6DzCOg3HbpfKZtvCNGI5gb69cAYrfXM+tc3AwO11nc3OGYk8DFGCz4bI9xTGjnXLGAWQIcOHfqlp6df0AWJ5ikoq2JXZiEOSuHkoHBwUDg6KByU4rUfDrEy5Tj/nRTPtX0iLFdUWT5sXQhb34aio+AZCH1uMm6s+lnoj4sQNqC5gX4DMPq0QE/UWt/T4Jh2QJ3WulQpNRZ4QWt91j3TpIVunSqqa5m+aAtbjhTw2rT+XNw9yLIF1NXCwe8gaaGxVrvWED0Khs0xWu9CtHHNnSmaCTS8YxWB0Qr/hda6WGtdWv/8S8BZKSXDF2yQm7MjC6b1o3uoN3cs2crW9ALLFuDgaMw8nfIuzEk2lu/NSYG3roI3r4SjGy1bjxA2pCmBvgWIUUp1Vkq5AJOB5Q0PUEqFKKVU/fPE+vPmt3SxwjK83Zx5c3oiIe3cmL5oC/uOm7Syok+4Eeh/3AljnoQTe2HhaHjnOsjaZk5NQlixcwa61roGuBtYBaQCH2itU5RSs5VSs+sPux5IVkrtBF4EJmuzBriLFhHg5cri2wbi5uzItIWbyCg4ZV4xzm4w6A4j2C99BLK2wmsXw7tTIe0bKG/lJQ6EsBEysUic1d7jxUx89Sf8vVx5Z+ZAwn3dzS4JKoph06uw4X9QWWy8F9AVIhIhcoDxGNhddlkSdkm2oBPNknSkgFsWbkYpxcPjYpk8IJL6HjZzVZUZrfWMzZC5xXgsr+/z9wqB+EmQcBMEdjW3TiFakAS6aLaMglM88NEufjqUz/CYAJ6Y0JuI9h5ml/VbWkP+QWO99tTPIe1rYxu9iAGQcCP0mmAsOyCEDZNAFy2irk6zdPNRnvjS2ObuoXGxTE3s8LvWem2dJqe4ggAvV1ycTOz2KM2FXe/D9iXGptdO7tDjahh6r7E6pBA2SAJdtKiMglPMXbaL9QfyGdrFn8FR/mQUlJNZeIqMgnKyC8upqdNEBXryyR1D8fFwNrdgrSF7mxHsu96HqlLoNhaG3w8R/cytTYjzJIEuWpzWmnc3Z/DYij2UVdUS4OVKpJ87Ee09iGzvjrebM899s4/B0QEsunUAjg5W0OcOcKoANr8Gm14xRsd0vghG3G/sj2oN9wWEOAcJdNFqyquMDTTcXRx/99m7m4/y4LLd3D4iigfHxlq6tLOrLIGkRfDTS1CaA+H9ocd4iLoYgnvJCBlhtc4W6K2wi7BoSxoL8p9NSexA6rFi5v9wiO6h3pZdG+ZcXL2NvvTEWbBjidFq/+bvxmceARB1kRHu0ReDjxXVLcRZSAtdtKrq2jpuen0T2zMK+Wj2YMsszXuhio/BobVwaI3xWJpjvB/cC2KvMn6CekjXjDCVdLkIU+WXVjL+pfXU1mmW3zOUIG83s0s6N60hNxUOroa9K+rXkNHgF1Uf7uMhrK90zQiLk0AXptuTXcx1r2wgNtSbd2cNwtXpzF01VqkkB/atMMa3H/4B6mrA0dXYacm3I7TvCL4djOdhCUbwC9EKJNCFVVix6xh3Ld3GhD7hPHldnLlj1Juj/KSxhkxOMpxMh8J0OHmkwZoyCnpeCyP+AsE9zKxU2CG5KSqswri4UA7kduW/3+5n7/ESnr0hnh5h7cwu6/y5t4e4icDE375fUWyEe/Iy2LwAUpYZXTMXPQAhvU0pVbQt0kIXFvd1ynEe+iSZovIq7h0Vw+yR0Tg72mhr/UxOFcDGebBpvrGAWPcrYci9RrC7WNmSCcKmSJeLsDony6r4+/IUPt+ZTe9wH/4zMZ6uwd5ml9Xyyk8aob5xHlQUGe95BUP7ztC+k7G9nl80dL1c1pkRTSKBLqzWil3H+L/PkimtqOG+y7py+4goHKxlVmlLqigy+t1PHjb62wuOGI/FWYAGFy9jAbGBt4N/tLm1CqsmgS6sWl5pJX/7JJmVKce5NDaI/05KwNvN5PVfLKW6Ao7vhi2vQ/LHxuiZmMuNDT2iRsqYd/E7EujC6mmtefundB79Yg9RAZ68Nq0/nQI8zS7LskpyIOkNY4PsshMQGAudhhpdNF7B4B3y66NnkIyBb6Mk0IXN2HAgjzuXbkNreGlqH4bHBJpdkuXVVBqt9aRFkJ/W+BZ7rj4QmQgdB0OHIRDWx9iqT9g9CXRhU47mn+IPbyeRllvCw+N6MGNoJ+vYIcksNZXGMgQlOfWPx4wx8Ok/Qd4+4xhHFwjvB5EDIbyv8bxduHTZ2CEJdGFzSitr+NP7O/h6Tw7X94vgsWt72d7sUksoy4eMjZC+wVie4NgOox8ejG6Z8L7GEgUdBkKnEdJNYwck0IVNqqvTPL86jRdXpzGwsx8LpvXHx72N3Cy9UNUVRus9a5uxqUfWNsjbD2hjYbHhfzZmsTrIH0dbJYEubNpnO7K4/8OdRAd68eb0REJ8pK/4vFQUw/5VsO5ZOLEX/LsYwd57IjjKZHFbI4EubN6PaXncvjgJXw8X3poxgC5BdjgJqbXV1UHqcvjhWcjZbSwkNuw+iLns3P3tlSWQmQSZW4z14eOnSP+8SSTQhV1Iziri1kVbqK6tY+Gt/enX0c/skmyT1rDvK/jhacjebrzn6gNBscZPcE8I7GbchM3YCBmbICcFdN2v5+h+JVz9krGujbAoCXRhN47mn+KWRZvJLiznpal9uaxHsNkl2S6tjVb3sR3G2u+5qZCb8usSBWDMYI3oD5GDjGGS4f2MHZ6++Tt4h8ENi4zPhcVIoAu7kl9ayYw3t7A7q4jpQztzbZ9weoa1a9tDG1uK1sawyBN7ja34gns2fgM1Mwk+nA4l2XDpIzD4LumCsRAJdGF3TlXV8OCy3azYdYyaOk1UoCdXxYUxPiGM6EAvs8trG8pPwmd3w94voOsVcM088JBusNYmgS7s1smyKr5KPs7nO7PZeDgfraFHaDtuHdqJif0jzS7P/mltrCb59d/A1QuiL4HoUcbm2u3Czu88pTnGuja5e4yZr51HtF7dNkwCXbQJOcUVrNh1jGXbM0nOKmbWiCjmjulun6s3WpvsHfDTy8bm2mW5xnsB3Yxg7zgEnNyMCU+//NRCTYUxRv54shHkp/J+e874qTD6MWn1n0YCXbQptXWaRz5P4e2f0rkmIYynr4+33e3ubI3WxoiYg9/BoTXGDNaaijMf7+hqjKwJ6QUhcRDcCwJijFb/+ueNUTRXPG1MhpI+ekACXbRBWmvmrT3IM6v2MTwmgFdu6oeXq0yisbjqCmPkDICD029/HJ2NkTJnmtx0fLfRR39sB3QbC+P+c37dOHZKAl20WR8mZTB32W5iQ71ZdGsigd6uZpckzkdtjbHb05rHjT8Ag+40tvEL6Grs9uR4hqUgKkuhKBNO5UNoHLjaz0Q0CXTRpq3Zm8udS7YR6O3KWzMS6dzW1lm3BwWH4Iv7jD76nzk4gV+UEe6egVBy3AjxogyoKPz1OEcX6DgUuo42fvyifnvuulrj/D/fkPWLsuplEZod6EqpMcALgCPwutb6yTMcNwDYCEzSWn90tnNKoAtL2n70JDPe3ALAc5MSuLhbkMkViQtSUWysEZ+XZtxQzdtvPC/NNZYv8IkAn58fI42JUUfWQdrX9YuUAf4x0OVSqC4zbsjmpkJNef0XKEAb+7xe/BD0nGB1K1Q2K9CVUo7AfuAyIBPYAkzRWu9p5LhvgApgoQS6sDaH88qYvXgr+3JKuH1EFPeP7oazo3X9YxWtqOAQ7P8a0lbBkR+NsA/pBcG9jQlUIb2MkTkHV8N3/zZa60E9YdTDRh++ldyUbW6gDwb+qbUeXf/6QQCt9ROnHTcHqAYGAF9IoAtrVFFdy6Nf7GHppqMkRPryvyl9iPTzMLssYWm1NcYM2DOFdF0dpCwz+u4LDhpLHiTeDu1CwcPf+HH3AycXy9ZN8wP9emCM1npm/eubgYFa67sbHBMOLAVGAW9whkBXSs0CZgF06NChX3p6+oVdkRDNtGLXMeZ+vAsUPH1dHFf0DjW7JGGNamtg51JY+xQUZ/7+cxdvY5y8hz94BhjLJXj4/fq8XajR9dMu3Jh41QLOFuhN6fVv7E/Y6X8Fngf+qrWuPdt6GlrrBcACMFroTfhuIVrFuLhQeof7cM+727hjyTZuGtSBh8f2wN1FNn4QDTg6Qd9pEDfZ6Ls/lQ+nCk57zDcmRZXmQM4e43ljY+/dfI2+/Xbh0Pt6iJvY4uU2JdAzgYZzqCOA7NOO6Q+8Vx/mAcBYpVSN1vrTFqlSiFbQwd+DD2cP4ZlVe3lt3WHWH8jnmevj6N9JZiaK0zi5GP3sTaE1VJ+CshNQnF0/8iYTirPqn2cZN3FbQVO6XJwwbopeAmRh3BSdqrVOOcPxbyJ96MLGbDiQx18+2kV2UTkzhnbm/su7SWtdWKWzdbmc8xa/1roGuBtYBaQCH2itU5RSs5VSs1u2VCHMMaRLAKvuG8GNAzvwxo+HGfviOpKOFJhdlhDnRSYWCXGa01vr913WVZYNEFajWS10Idqa01vrgx5fzT+Xp3DwRKnZpQlxVtJCF+IsdmUWsmj9EVbsOkZVbR3DYwK4dUgnRnYLwlGW5RUmkLVchGimEyWVvLf5KEs2HeV4cQUd/Dx4aGwsY3qFmF2aaGOky0WIZgr0duWeS2JY99eLmXdjX7xcnbj33e1sTT9pdmlC/EICXYjz4OzowNjeoSz9w0BCfNyY/c5WjhedZQMHISxIAl2IC+Dr4cLrt/TnVGUNty9OoqK61uyShJBAF+JCdQ325rlJCezMLOKhT3Zj1v0oIX4mgS5EM4zuGcJ9l3Zl2bYsFq4/YnY5oo2TQBeime4Z1YUxPUN4bMUefkzLO/cvCNFKJNCFaCYHB8V/JsYTE+TNXUu3kZ5fZnZJoo2SQBeiBXi6OvHatP4oBde8vJ45723nw6QMsgvLz/3LQrQQWaBCiBbSwd+Dt2ck8saPh/nxQB6f7jBWmY4K9GRodAAXdQ3k4u4yw1S0HpkpKkQr0FqzL6eEH9PyWH8gj02HCzhVVUvnAE9uHxHFtX3DcXWS5XnF+ZOp/0KYrKqmjm9Tc5i39gDJWcUEt3Nl5rAopgzsICs5ivMigS6EldBa8+OBPOatOchPh/LxcXfmliGduOOiaNlQQzRJc/cUFUK0EKUUw2MCGR4TyPajJ3ll7UFeXJ3Gj2kneOOWAbT3tPwu8sJ+yCgXIUzSp0N7Fkzrz7wb+5KcXcz1r24g8+Qps8sSNkwCXQiTje0dyuIZieSWVDJh3gb2ZBebXZKwURLoQliBgVH+fDR7CA5KMWn+T2w4KDNOxfmTQBfCSnQL8WbZnUMI8XHj1oVb+HxnttklCRsjgS6EFQnzdeej2UNIiPTlnneN2aZCNJUEuhBWxsfDmbdvS2RItD8Pf5pMSnaR2SUJGyGBLoQVcnN25MUpffB1d+auJdsoqag2uyRhAyTQhbBSAV6u/G9KH44WnGLux7KBhjg3CXQhrNjAKH/uH92NFbuPsXhjutnlCCsngS6ElZs9IpqLuwXy7y9S2ZVZaHY5wopJoAth5RwcFM9NTCDAy4W7lm6jqFz600XjJNCFsAHtPV3439S+HCus4C8f7pT+dNEoCXQhbES/ju2Ze0V3vt6Tw5Nf7eVUVY3ZJQkrI4EuhA25bVhnJvQJZ/4Phxj+1BoW/HBQgl38QgJdCBuilOK5SQl8NHswPcLa8fiXexn+1Brmfy/BLmSDCyFs2tb0Ap7/No11aXn4ebpw58hopg/tLPuW2rGzbXDRpBa6UmqMUmqfUuqAUmpuI59frZTapZTaoZRKUkoNa27RQohz69fRj8W3DeTjO4bQM6wd/16RyrSFm8gtqTC7NGGCc7bQlVKOwH7gMiAT2AJM0VrvaXCMF1CmtdZKqTjgA61197OdV1roQrQsrTUfJGXwj+UpeLk68dzEBEZ0DTS7LNHCmttCTwQOaK0Paa2rgPeAqxseoLUu1b/+ZfAEZEyVEBamlGLSgA4sv3sYfp4uTFu4madW7qW6ts7s0oSFNCXQw4GGa3hm1r/3G0qpa5VSe4EVwIzGTqSUmlXfJZN04sSJC6lXCHEOXYO9+eyuYUxJjOSVtQeZvGAjWYXl53WOY0XlVNbUtlKForU0pcvlBmC01npm/eubgUSt9T1nOH4E8Het9aVnO690uQjR+pbvzOahZcbCXhd1C6zfoDqAiPYevzlOa01yVjGrUh/kGuUAAAxFSURBVI6zKuU4abmlRAd6Mv/mfnQJ8japetGYs3W5ODXh9zOByAavI4AzbqWitf5BKRWtlArQWss+WkKYaHx8GPERPry85gA/7M/jy93HAYgK8GR4TAB9O7Zn+9FCvtmTQ1ZhOQ4KEjv7cVV8V97acITxL63n6evjuDIuzOQrEU3RlBa6E8ZN0UuALIybolO11ikNjukCHKy/KdoX+ByI0Gc5ubTQhbAsrTUHcktZl5bHurQTbDxUQHl1LS5ODoyICeDyniFcGhuMn6cLAMeLKrhzyVa2HS1k+tBOPHhFLC5OMnXFbM1qoWuta5RSdwOrAEdgodY6RSk1u/7zV4HrgGlKqWqgHJh0tjAXQlieUoqYYG9igr2ZMawzlTW17DteQnSgF56uv4+CEB833ps1mCe+SmXR+iPsyizi5al9CfFxM6F60RQysUgIcU7Ld2Yz9+NdeLg48vLUvgyM8je7pDar2ROLhBBt2/j4MD67ayjt3J2Z+XYSmSdPmV2SaIQEuhCiSWKCvXnz1kS0hj99sJPaOulVtTYS6EKIJuvg78E/x/dk8+ECXv3+oNnliNNIoAshzst1fcMZ1zuU/36zX7bEszIS6EKI86KU4rFrexHo7cqc93bIsr1WRAJdCHHefD1c+M/EeA7nl/HvFalmlyPqSaALIS7IkOgAZg2PYummo3yzJ8fscgQS6EKIZvjT5V3pEdqOv368S9ZgtwIS6EKIC+bq5MiLUxIoq6xhzns7KCirMrukNk0CXQjRLF2CvPnX1b3YdLiAkc+s4Y0fD8sa7CaRQBdCNNvEAZF89cfhxEf68q8v9jD6+R9Ysy/X7LLaHAl0IUSL6BrszdszEnnjlv5oDdMXbeHWRZs5kFtqdmlthgS6EKLFKKW4JDaYVXNG8LdxsWw9cpIxz//A6+sOIQuwtj4JdCFEi3NxcmDm8CjW/GUko7oH8e8Vqdy+eCtF5dVml2bXJNCFEK0mwMuV+Tf342/jYvluby5X/e9HkrOKzC7LbkmgCyFalVKKmcOjeP/2wVTX1jHhlQ0s2ZQuXTCtQAJdCGER/Tq2Z8W9wxkc5c/DnyQz5/0dsq56C5Mdi4QQFlVXp5m39gDPfbOfOg3RgZ6M6BrIRV0DGRTlj5uzo9klWrWz7VgkgS6EMMXhvDK+25vL9/tPsOlQPpU1dbg6OZDY2Y/hMQEMiQ4gNrQdjg7K7FKtigS6EMKqVVTXsulwAd/vO8H3+3M5eKIMAB93ZwZF+TEkOoAh0f50CfJCqbYd8GcL9N9v9S2EEBbm5uzIRfXdLtCDnOIKfjqYz4aDeWw4mM+qFGM1x7gIHxbdOgB/L1dzC7ZS0kIXQli9jIJTrNmXy2MrUuno78GSmYMI9G6boX62FrqMchFCWL1IPw+mDe7EoukDyCgoZ/KCn8gtluV6TyeBLoSwGUOiA3hrRiLHiyqYtGAjx4rKzS7JqkigCyFsSmJnP96+LZG8kkomzd8oY9kbkEAXQticfh39WDxzIIWnqpg0fyNH8yXUQQJdCGGjEiJ9WfqHQZRV1TDhlQ3M//5gm1/8SwJdCGGzeoX78N6sQUQHevLEV3sZ/MRq/vFZMofzyswuzRQybFEIYReSs4pYuP4wn+/MpqZOc0n3IG4bFsXgaH+zS2tRMlNUCNFm5JZU8M5P6byz6SgFZVU8NLY7s0ZEm11Wi5Fx6EKINiPI240/Xd6NDXNHMa53KI9/uZfPdmSZXZZFyNR/IYRdcnN25D8T48krreT+D3cS4OXK0C4BZpfVqqSFLoSwW27OjiyY1p+oAC9uX7yVlGz73i2pSYGulBqjlNqnlDqglJrbyOc3KqV21f9sUErFt3ypQghx/nzcnXlzxgC83Zy4ddEWMgrsd8z6OQNdKeUIvAxcAfQApiilepx22GHgIq11HPAvYEFLFyqEEBcq1Medt2YkUlldyy2LNnOyrMrsklpFU1roicABrfUhrXUV8B5wdcMDtNYbtNYn619uBCJatkwhhGiersHevH7LADJPlnPbW1soq6wxu6QW15RADwcyGrzOrH/vTG4DvmrsA6XULKVUklIq6cSJE02vUgghWkBiZz9emJTA9oxCBj6+mr9+tIvNhwvsZsPqpoxyaWx7kEavXil1MUagD2vsc631Auq7Y/r3728f/wsKIWzKFb1D+Wj2EN7dfJTPd2XzflIGkX7uTOgTwXV9I+jg72F2iResKYGeCUQ2eB0BZJ9+kFIqDngduEJrnd8y5QkhRMvr17E9/Tq259Gre7Iy+Tgfb8vkxe/SeGF1Gh39PQj1cSPMx51QXzdCfNwJ83Gje2g7wn3dzS79rJoS6FuAGKVUZyALmAxMbXiAUqoDsAy4WWu9v8WrFEKIVuDh4sSEvhFM6BtBdmE5n+3IJiW7iONFFWw6XMDx4gpq64zOBAcF1ySEc/eoLkQFeplceePOGeha6xql1N3AKsARWKi1TlFKza7//FXg74A/MK9+A9eaM01NFUIIaxTm684dI3+7REBtneZESSXZReV8tfsYizem8+mOLK6uD/ZoKwt2WctFCCGa6ERJJa+tO8Tin9KprKllfHwY91wSY9Fgl8W5hBCiBeWVVvLaD4d4uz7Yr4oP455RMXQJav1gl0AXQohWkF9ayYL6Fnt5dS1XxYVx7yVd6BLk3WrfKYEuhBCtKL+0ktfWHebtn45QXl3LlXFh3DuqCzHBLR/sEuhCCGEBBWVVvLbuEG9vOEJZVS0DO/txTZ9wrugVgq+HS4t8hwS6EEJYUEFZFUs2pvPJjiwOnSjD2VExslsQ1ySEc0lsEG7Ojhd8bgl0IYQwgdaalOxiPt2exfKd2eSWVOLl6sScS2OYOTzqgs55tkCXDS6EEKKVKKXoFe5Dr3AfHhwby6ZD+Xy6I4sQH7dW+T4JdCGEsABHB8WQLgEMacVdk2THIiGEsBMS6EIIYSck0IUQwk5IoAshhJ2QQBdCCDshgS6EEHZCAl0IIeyEBLoQQtgJ06b+K6VOAOkX+OsBQF4LlmNL2uq1y3W3LXLdZ9ZRax3Y2AemBXpzKKWS2uoWd2312uW62xa57gsjXS5CCGEnJNCFEMJO2GqgLzC7ABO11WuX625b5LovgE32oQshhPg9W22hCyGEOI0EuhBC2AmbC3Sl1Bil1D6l1AGl1Fyz62ktSqmFSqlcpVRyg/f8lFLfKKXS6h/bm1lja1BKRSql1iilUpVSKUqpP9a/b9fXrpRyU0ptVkrtrL/uR+rft+vr/plSylEptV0p9UX9a7u/bqXUEaXUbqXUDqVUUv17zbpumwp0pZQj8DJwBdADmKKU6mFuVa3mTWDMae/NBVZrrWOA1fWv7U0N8GetdSwwCLir/r+xvV97JTBKax0PJABjlFKDsP/r/tkfgdQGr9vKdV+stU5oMPa8WddtU4EOJAIHtNaHtNZVwHvA1SbX1Cq01j8ABae9fTXwVv3zt4BrLFqUBWitj2mtt9U/L8H4Rx6OnV+7NpTWv3Su/9HY+XUDKKUigHHA6w3etvvrPoNmXbetBXo4kNHgdWb9e21FsNb6GBjBBwSZXE+rUkp1AvoAm2gD117f7bADyAW+0Vq3iesGngceAOoavNcWrlsDXyultiqlZtW/16zrtrVNolUj78m4SzuklPICPgbmaK2LlWrsP7190VrXAglKKV/gE6VUL7Nram1KqSuBXK31VqXUSLPrsbChWutspVQQ8I1Sam9zT2hrLfRMILLB6wgg26RazJCjlAoFqH/MNbmeVqGUcsYI8yVa62X1b7eJawfQWhcCazHuodj7dQ8FxiuljmB0oY5SSr2D/V83Wuvs+sdc4BOMLuVmXbetBfoWIEYp1Vkp5QJMBpabXJMlLQduqX9+C/CZibW0CmU0xd8AUrXWzzX4yK6vXSkVWN8yRynlDlwK7MXOr1tr/aDWOkJr3Qnj3/N3WuubsPPrVkp5KqW8f34OXA4k08zrtrmZokqpsRh9bo7AQq31YyaX1CqUUu8CIzGW08wB/gF8CnwAdACOAjdorU+/cWrTlFLDgHXAbn7tU30Iox/dbq9dKRWHcRPMEaOh9YHW+lGllD92fN0N1Xe53K+1vtLer1spFYXRKgej63up1vqx5l63zQW6EEKIxtlal4sQQogzkEAXQgg7IYEuhBB2QgJdCCHshAS6EELYCQl0IYSwExLoQghhJ/4fh7diqq5TeNEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(th)), th)\n",
    "plt.plot(range(len(th)), vh)\n",
    "plt.savefig('loss_graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./resources/tweet_embeddings.pickle\", \"rb\") as pkl_in:\n",
    "    feat_emb = pkl.load(pkl_in)\n",
    "    \n",
    "feat_emb=np.asarray([emb.detach().numpy() for emb in feat_emb])\n",
    "feat_emb.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covidfake",
   "language": "python",
   "name": "covidfake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
