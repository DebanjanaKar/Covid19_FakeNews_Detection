{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading english data\n",
    "import pickle as pkl\n",
    "with open('./resources/covid_en_tweet.pickle', 'rb') as pkl_in:\n",
    "    tweets_en = pkl.load(pkl_in)\n",
    "#loading bengali data\n",
    "with open('./resources/covid_bn_tweet.pickle', 'rb') as pkl_in:\n",
    "    tweets_bn = pkl.load(pkl_in)\n",
    "with open('./resources/covid_bn_tweet_test.pickle', 'rb') as pkl_in:\n",
    "    tweets_bn_test = pkl.load(pkl_in)\n",
    "#loading hindi data\n",
    "with open('./resources/covid_hi_tweet.pickle', 'rb') as pkl_in:\n",
    "    tweets_hi = pkl.load(pkl_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyphrase(tweets):\n",
    "    from rake_nltk import Rake\n",
    "    import string\n",
    "    \n",
    "    r = Rake(min_length=2, max_length=4)   # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "    keyphrases = []\n",
    "    keywords = []\n",
    "    for txt in tweets:\n",
    "        if type(txt) == str:\n",
    "            txt = ''.join(x for x in txt if x in string.printable)\n",
    "            keywords.append(r.extract_keywords_from_text(txt))\n",
    "            if len(r.get_ranked_phrases()) > 2:\n",
    "                keyphrases.append(r.get_ranked_phrases()[:3])\n",
    "            else:\n",
    "                keyphrases.append([txt])\n",
    "            \n",
    "    assert len(keyphrases) == len(tweets)\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_keyphrases = get_keyphrase(tweets_en['text'])\n",
    "with open('./resources/en_keyphrases.txt', 'w') as f:\n",
    "    for item in en_keyphrases:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['পরীক্ষার ফলাফল', 'ধনীমানুষ', 'best way'],\n",
       " ['এই বকশির অভিনয় হয়'],\n",
       " ['অবসর সময়', 'government instructions', 'finish knocking'],\n",
       " ['কোনও অপরাধ নয় তবে এপ্রিলের আগে সফল হওয়ার আগে করোনার ভাইরাস অদৃশ্য হয়ে গেল'],\n",
       " ['please social distance', 'ব্যক্তিগত প্রতিরক্ষামূলক সরঞ্জাম', 'feel broken'],\n",
       " ['need boris', 'morally right', 'covid19 benice'],\n",
       " ['fast food workers', 'dollar general workers', 'walmart employees'],\n",
       " ['treating patients infected', 'usama riaz', 'stubborn anyway'],\n",
       " ['corona shit passes', 'zombie apocalypse', 'kids know'],\n",
       " ['Shortness of breath is therefore a symptom of Corona, but did you know that it also gives me shortness of breath? Anxiety. And you know what worries me? I think I have corona.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "trans_bn_1 = [translator.translate(txt, dest='en').text for txt in tweets_bn['text']]\n",
    "bn_keyphrases1 = get_keyphrase(trans_bn_1)\n",
    "bn_keyphrases1 = [[translator.translate(phrase, dest='bn').text for phrase in line] for line in bn_keyphrases1]\n",
    "bn_keyphrases1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n"
     ]
    }
   ],
   "source": [
    "bn_keyphrases1 = [ phr if len(phr) > 2 else [list(tweets_bn['text'])[i]] for i, phr in enumerate(bn_keyphrases1)]\n",
    "print(len(bn_keyphrases1))\n",
    "with open('./resources/bn_keyphrases1.txt', 'w') as f:\n",
    "    for item in bn_keyphrases1:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mohfw_india covid19india covid__19 pic',\n",
       "  'বুধবার সকাল পর্যন্ত দেশ',\n",
       "  'গত 24 ঘন্টা'],\n",
       " ['ওলক্লোকাটাপলিসে করোনার আক্রমণের সংখ্যা হ্রাস পেয়ে। এ নেমেছে তাদের মধ্যে ২৯ জন উদ্ধার পেয়েছেন ... কোভিড ১৯ কর্নাভাইরাসিক কলকাতার করোনপোসিটিভ সাবধানতা স্পষ্টতাত্ত্বিক সত্যিকারের নিউজ আইসামি ইউআরএল।'],\n",
       " ['ক্যালকোটুনিভার্টিস্টি রিওয়ার্সেক্সেক্সাম কোভিড__19 মাতামাটা',\n",
       "  'দুই মাস',\n",
       "  'এক মাস'],\n",
       " ['হার্ট ছবি', 'নারী ডাক্তার', 'কর্তব্য'],\n",
       " ['যাত্রীদের কোন রাজ্যে কোয়ারান্টিনে থাকতে হবে? করোনাভাইরাস-সর্বশেষ-সংবাদ URLটি সন্ধান করুন'],\n",
       " ['গত 24 ঘন্টা', 'মুকুট ঘর', '170 জন'],\n",
       " ['19 আক্রান্ত দেশ', 'সুস্থতার হার', 'jpnadda com'],\n",
       " ['মা, এই কোভিড -19 থেকে সবাইকে বাঁচান ... pic.twitter.com/3aqntqaoaq'],\n",
       " ['কেনাকাটা করতে যাবেন না। গোলরক্ষক রামসডালে আঘাত করবেন না।'],\n",
       " ['@ ডান্ডালে কোনও করোনার ইতিবাচক লোক নেই, গুজব ছড়াবেন না।']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_bn_2 = [translator.translate(txt, dest='en').text for txt in tweets_bn_test['text']]\n",
    "bn_keyphrases2 = get_keyphrase(trans_bn_2)\n",
    "bn_keyphrases2 = [[translator.translate(phrase, dest='bn').text for phrase in line] for line in bn_keyphrases2]\n",
    "bn_keyphrases2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "bn_keyphrases2 = [ phr if len(phr) > 2 else [list(tweets_bn_test['text'])[i]] for i, phr in enumerate(bn_keyphrases2)]\n",
    "print(len(bn_keyphrases2))\n",
    "with open('./resources/bn_keyphrases2.txt', 'w') as f:\n",
    "    for item in bn_keyphrases2:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['अमीर व्यक्ति', 'प्रबंधित परिणाम', 'सबसे अच्छा तरीका'],\n",
       " ['यह बकवास है'],\n",
       " ['दो बच्चे पालन नहीं कर सकते', 'कृपया पालन करें', 'काम किया'],\n",
       " ['कोई अपराध नहीं है लेकिन अप्रैल से पहले गायब हुआ कोरोना वायरस वास्तव में बेकार है'],\n",
       " ['आत्म अलगाव covid19 url', 'सामाजिक भेदभाव', 'टूटा हुआ महसूस करना'],\n",
       " ['बोरिस की जरूरत है', 'नैतिक रूप से सही है', 'covid19 बेनिस'],\n",
       " ['फास्ट फूड कार्यकर्ता',\n",
       "  'डॉलर के सामान्य कार्यकर्ता',\n",
       "  'कार्यकर्ताओं को कचरा'],\n",
       " ['पीड़ित मरीजों की जांच', 'उसमे रियाज़', 'वैसे भी बनी रही'],\n",
       " ['When this corona shit passes we have to promise each other that we are going to tell our children that we survived a zombie apocalypse in 2020'],\n",
       " ['So shortness of breath is a corona symptom, but you know I also have shortness of breath? anxiety. And you know what worries have been given to me? Thinking that I have a corona.']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "trans_hi = [translator.translate(txt, dest='en').text for txt in tweets_hi['text'] if type(txt) == str]\n",
    "hi_keyphrases = get_keyphrase(trans_hi)\n",
    "hi_keyphrases = [[translator.translate(phrase, dest='hi').text for phrase in line] for line in hi_keyphrases]\n",
    "hi_keyphrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_keyphrases = [ phr if len(phr) > 2 else [list(tweets_hi['text'])[i]] for i, phr in enumerate(hi_keyphrases)]\n",
    "with open('./resources/hi_keyphrases.txt', 'w') as f:\n",
    "    for item in hi_keyphrases:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed the keyphrases :D\n",
    "from flair.embeddings import TransformerDocumentEmbeddings #uncomment this & next 2 lines once, after that comment again once loaded.\n",
    "from flair.data import Sentence\n",
    "embedding = TransformerDocumentEmbeddings('bert-base-multilingual-uncased')\n",
    "\n",
    "def get_embedding(sents):\n",
    "    embeds = []\n",
    "    \n",
    "    for line in sents:\n",
    "        phrase = ''\n",
    "        for phr in line:\n",
    "            phrase += phr + '. '\n",
    "        sentence = Sentence(phrase)\n",
    "        embedding.embed(sentence)\n",
    "        embeds.append(sentence.embedding)\n",
    "    \n",
    "    assert len(sents) == len(embeds)    \n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeds = get_embedding(en_keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_embeds1 = get_embedding(bn_keyphrases1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_embeds2 = get_embedding(bn_keyphrases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_embeds = get_embedding(hi_keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debanjana_ibm/anaconda3/envs/covidfake/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('./resources/keyphrases_embeddings.pickle', 'wb') as pkl_out:\n",
    "    pkl.dump(en_embeds, pkl_out)\n",
    "    pkl.dump(bn_embeds1, pkl_out)\n",
    "    pkl.dump(bn_embeds2, pkl_out)\n",
    "    pkl.dump(hi_embeds, pkl_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covidfake",
   "language": "python",
   "name": "covidfake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
